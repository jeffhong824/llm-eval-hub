# ğŸ§  LLM Evaluation Hub

> **ä¸€å€‹å®Œæ•´çš„ RAG & Agent ç³»çµ±è‡ªå‹•åŒ–æ¸¬è©¦å¹³å°**  
> å¾æƒ…å¢ƒæè¿°åˆ°è©•ä¼°ï¼Œåªéœ€ç°¡å–®é»æ“Šï¼

## ğŸ“– é€™å€‹å°ˆæ¡ˆæ˜¯ä»€éº¼ï¼Ÿ

LLM Evaluation Hub æ˜¯ä¸€å€‹å°ˆç‚º **RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰** å’Œ **Agentï¼ˆå°è©±ä»£ç†ï¼‰** ç³»çµ±è¨­è¨ˆçš„å…¨è‡ªå‹•åŒ–æ¸¬è©¦å¹³å°ã€‚

### ğŸ¯ ä¸»è¦ç›®æ¨™

è§£æ±º LLM æ‡‰ç”¨é–‹ç™¼ä¸­æœ€ç—›çš„å•é¡Œï¼š**å¦‚ä½•æ¸¬è©¦ä½ çš„ RAG/Agent ç³»çµ±ï¼Ÿ**

å‚³çµ±æ–¹å¼éœ€è¦ï¼š
- âŒ æ‰‹å‹•ç·¨å¯«å¤§é‡æ¸¬è©¦å•é¡Œ
- âŒ è‡ªå·±æƒ³åƒå„ç¨®ç”¨æˆ¶è§’è‰²
- âŒ äººå·¥æª¢æŸ¥æ¯å€‹å›ç­”
- âŒ é›£ä»¥æ¨¡æ“¬çœŸå¯¦ç”¨æˆ¶è¡Œç‚º

**æˆ‘å€‘çš„è§£æ±ºæ–¹æ¡ˆï¼š**
- âœ… **è‡ªå‹•ç”ŸæˆçœŸå¯¦ç”¨æˆ¶è§’è‰²**ï¼šåªéœ€æè¿°ä½¿ç”¨æƒ…å¢ƒï¼Œç³»çµ±è‡ªå‹•ç”Ÿæˆ 10+ å€‹è¶…è©³ç´°çš„ç”¨æˆ¶è§’è‰²
- âœ… **è‡ªå‹•ç”Ÿæˆæ¸¬è©¦é›†**ï¼šåŸºæ–¼è§’è‰²ç‰¹å¾µç”ŸæˆçœŸå¯¦çš„å•é¡Œæˆ–ä»»å‹™
- âœ… **è‡ªå‹•åŒ–è©•ä¼°**ï¼šä½¿ç”¨ LLM Judge å’Œ RAGAS æŒ‡æ¨™è‡ªå‹•è©•ä¼°ç³»çµ±è¡¨ç¾
- âœ… **æ™ºèƒ½å°è©±æ¨¡æ“¬**ï¼šAgent æ¸¬è©¦ä¸­ï¼ŒLLM è‡ªå‹•æ‰®æ¼”ç”¨æˆ¶é€²è¡Œå¤šè¼ªå°è©±

### ğŸ”„ å®Œæ•´å·¥ä½œæµç¨‹

```
æƒ…å¢ƒæè¿°
    â†“
è‡ªå‹•ç”Ÿæˆ 10+ å€‹çœŸå¯¦ç”¨æˆ¶è§’è‰²
    â†“
è‡ªå‹•ç”ŸæˆçŸ¥è­˜æ–‡ä»¶
    â†“
é¸æ“‡æ¸¬è©¦æ¨¡å¼ï¼ˆRAG / Agentï¼‰
    â†“
è‡ªå‹•ç”Ÿæˆæ¸¬è©¦é›†ï¼ˆQA Pairs / ä»»å‹™å ´æ™¯ï¼‰
    â†“
è‡ªå‹•åŒ–è©•ä¼°ï¼ˆRAGAS æŒ‡æ¨™ / å°è©±æ¸¬è©¦ï¼‰
```

### ğŸ’¡ ä½¿ç”¨ç¯„ä¾‹

**æƒ…å¢ƒ**: "ä¸€å€‹æˆ¿åœ°ç”¢åª’åˆå¹³å°"

**ç³»çµ±è‡ªå‹•ç”Ÿæˆ**:
- ğŸ‘¤ 10+ å€‹çœŸå¯¦ç”¨æˆ¶è§’è‰²
  - é™³å°ç¾ï¼ˆ32æ­²è»Ÿé«”å·¥ç¨‹å¸«ï¼Œé¤Šè²“ï¼Œæƒ³åœ¨å…§æ¹–è²·æˆ¿ï¼Œé ç®—2000è¬...ï¼‰
  - ç‹å¤§å¼·ï¼ˆ45æ­²ä¼æ¥­ä¸»ï¼Œéœ€è³£å±‹æ›å±‹ï¼Œé‡è¦–å­¸å€...ï¼‰
  - ...
- ğŸ“š 15+ ä»½çŸ¥è­˜æ–‡ä»¶
  - è³¼æˆ¿æµç¨‹æŒ‡å—ã€å„å€æˆ¿åƒ¹åˆ†æã€è²¸æ¬¾ç”³è«‹æŒ‡å—...
- ğŸ” 30+ å€‹çœŸå¯¦å•é¡Œ
  - "æˆ‘å’Œæˆ‘çš„è²“å’ªæƒ³æ‰¾é è¿‘å…§æ¹–ç§‘æŠ€åœ’å€çš„æˆ¿å­..."ï¼ˆé™³å°ç¾çš„å•é¡Œï¼‰
  - "æˆ‘æƒ³è³£æ‰ç¾æœ‰æˆ¿å­æ›æ›´å¤§çš„ï¼Œå°å­©è¦ä¸Šå°å­¸äº†..."ï¼ˆç‹å¤§å¼·çš„å•é¡Œï¼‰

**ç„¶å¾Œè‡ªå‹•è©•ä¼°ä½ çš„ç³»çµ±å›ç­”è³ªé‡ï¼**

## ğŸš€ å¿«é€Ÿé–‹å§‹ï¼ˆ3 æ­¥é©Ÿï¼‰

### æ­¥é©Ÿ 1: æº–å‚™ç’°å¢ƒ

ç¢ºä¿å·²å®‰è£ï¼š
- [Docker Desktop](https://www.docker.com/products/docker-desktop)
- OpenAI API Keyï¼ˆå¿…é ˆï¼‰
- Gemini API Keyï¼ˆé¸å¡«ï¼‰

### æ­¥é©Ÿ 2: å•Ÿå‹•æœå‹™

```bash
# 1. Clone å°ˆæ¡ˆ
git clone <repository-url>
cd llm-eval-hub

# 2. å‰µå»º .env æ–‡ä»¶ï¼ˆå¡«å…¥ä½ çš„ API keysï¼‰
cp env.example .env
# ç·¨è¼¯ .envï¼Œå¡«å…¥ä½ çš„ OPENAI_API_KEY

# 3. å•Ÿå‹• Docker æœå‹™
docker-compose up -d

# 4. æŸ¥çœ‹æ—¥èªŒï¼ˆç¢ºèªå•Ÿå‹•æˆåŠŸï¼‰
docker-compose logs -f app
```

### æ­¥é©Ÿ 3: é–‹å§‹ä½¿ç”¨

æ‰“é–‹ç€è¦½å™¨è¨ªå•ï¼š

**ğŸ¨ Web ç•Œé¢ï¼ˆæ¨è–¦æ–°æ‰‹ï¼‰**
- å®Œæ•´å·¥ä½œæµç¨‹: http://localhost:8000/static/workflow.html
- ä¸»æ§å°: http://localhost:8000

**ğŸ“š API æ–‡æª”**
- Swagger UI: http://localhost:8000/docs

## ğŸ¬ ä½¿ç”¨æ–¹å¼

### æ–¹å¼ 1: Web ç•Œé¢ï¼ˆæœ€ç°¡å–®ï¼‰

1. è¨ªå• http://localhost:8000/static/workflow.html
2. **éšæ®µ 1**: è¼¸å…¥ä½¿ç”¨æƒ…å¢ƒ â†’ è‡ªå‹•ç”Ÿæˆè§’è‰²
3. **éšæ®µ 2**: è‡ªå‹•ç”ŸæˆçŸ¥è­˜æ–‡ä»¶
4. **éšæ®µ 3**: é¸æ“‡ RAG æˆ– Agent æ¨¡å¼
5. **éšæ®µ 4**: è‡ªå‹•ç”Ÿæˆæ¸¬è©¦é›†
6. **éšæ®µ 5**: åŸ·è¡Œè©•ä¼°

### æ–¹å¼ 2: API èª¿ç”¨

```python
import requests

BASE_URL = "http://localhost:8000/api/v1/testset"

# ç”Ÿæˆè§’è‰²
response = requests.post(f"{BASE_URL}/workflow/generate-personas", json={
    "scenario_description": "ä½ çš„ä½¿ç”¨æƒ…å¢ƒæè¿°...",
    "num_personas": 10,
    "output_folder": "/app/outputs/my_test",
    "model_provider": "openai",
    "model_name": "gpt-4"
})

# ç¹¼çºŒä¸‹ä¸€å€‹éšæ®µ...
```

## âœ¨ æ ¸å¿ƒåŠŸèƒ½

| åŠŸèƒ½ | èªªæ˜ | é©ç”¨å ´æ™¯ |
|------|------|----------|
| **ğŸ­ æ™ºèƒ½è§’è‰²ç”Ÿæˆ** | æ ¹æ“šæƒ…å¢ƒè‡ªå‹•ç”Ÿæˆè¶…è©³ç´°ç”¨æˆ¶è§’è‰² | äº†è§£æ½›åœ¨ç”¨æˆ¶æ˜¯èª° |
| **ğŸ“š æ–‡ä»¶ç”Ÿæˆ** | è‡ªå‹•ç”Ÿæˆå¤šæ¨£åŒ–çŸ¥è­˜æ–‡ä»¶ | å»ºç«‹ RAG çŸ¥è­˜åº« |
| **ğŸ” RAG æ¸¬è©¦é›†** | ç”Ÿæˆè§’è‰²åŒ–çš„å•ç­”å° | æ¸¬è©¦ RAG ç³»çµ± |
| **ğŸ¤– Agent æ¸¬è©¦é›†** | ç”Ÿæˆä»»å‹™å ´æ™¯ | æ¸¬è©¦å°è©± Agent |
| **ğŸ’¬ æ™ºèƒ½å°è©±** | LLM æ‰®æ¼”ç”¨æˆ¶å¤šè¼ªå°è©± | Agent å£“åŠ›æ¸¬è©¦ |
| **ğŸ“Š è‡ªå‹•è©•ä¼°** | RAGAS æŒ‡æ¨™ + LLM Judge | é‡åŒ–ç³»çµ±è¡¨ç¾ |

## ğŸ“ è¼¸å‡ºçµæ§‹

```
outputs/
â””â”€â”€ your_scenario/
    â”œâ”€â”€ 01_personas/              # è§’è‰²æª”æ¡ˆ
    â”‚   â”œâ”€â”€ persona_001_é™³å°ç¾.md
    â”‚   â”œâ”€â”€ personas_summary.xlsx
    â”‚   â””â”€â”€ personas_full.json
    â”œâ”€â”€ 02_documents/             # çŸ¥è­˜æ–‡ä»¶
    â”‚   â”œâ”€â”€ doc_001_è³¼æˆ¿æŒ‡å—.txt
    â”‚   â””â”€â”€ metadata.json
    â”œâ”€â”€ 03_rag_testset/          # RAG æ¸¬è©¦é›†
    â”‚   â””â”€â”€ rag_testset.xlsx
    â”œâ”€â”€ 03_agent_testset/        # Agent æ¸¬è©¦é›†
    â”‚   â””â”€â”€ agent_testset.xlsx
    â””â”€â”€ 04_evaluation/           # è©•ä¼°çµæœ
        â”œâ”€â”€ results.xlsx
        â””â”€â”€ conversation_logs/
```

## ğŸ“š æ–‡æª”

- ğŸ“– **[å®Œæ•´å·¥ä½œæµç¨‹æŒ‡å—](docs/WORKFLOW_GUIDE.md)** - è©³ç´°ä½¿ç”¨èªªæ˜
- ğŸ³ **[Docker é…ç½®æŒ‡å—](docs/DOCKER_VOLUME_SETUP.md)** - æœ¬åœ°è³‡æ–™å¤¾è¨ªå•è¨­å®š
- ğŸ“ **[å¯¦æ–½ç¸½çµ](docs/IMPLEMENTATION_SUMMARY.md)** - æŠ€è¡“ç´°ç¯€

## ğŸ”§ å¸¸è¦‹å•é¡Œ

### Q: éœ€è¦å“ªäº› API Keysï¼Ÿ
A: è‡³å°‘éœ€è¦ OpenAI API Keyã€‚Gemini å’Œ Hugging Face æ˜¯é¸å¡«ã€‚

### Q: ç”Ÿæˆä¸€æ¬¡æ¸¬è©¦é›†è¦å¤šå°‘éŒ¢ï¼Ÿ
A: ä½¿ç”¨ GPT-4 å®Œæ•´æµç¨‹ï¼ˆ10 è§’è‰² + 10 æ–‡ä»¶ + 30 æ¸¬è©¦ï¼‰ç´„ $3.50

### Q: æ”¯æ´ä¸­æ–‡å—ï¼Ÿ
A: å®Œå…¨æ”¯æŒç¹é«”ä¸­æ–‡å’Œç°¡é«”ä¸­æ–‡

### Q: å¯ä»¥ç”¨æœ¬åœ°æ¨¡å‹å—ï¼Ÿ
A: æ”¯æŒ Ollama æœ¬åœ°æ¨¡å‹

### Q: å¦‚ä½•è¨ªå•æœ¬åœ°è³‡æ–™å¤¾ï¼Ÿ
A: åƒè€ƒ [Docker Volume é…ç½®æŒ‡å—](docs/DOCKER_VOLUME_SETUP.md)

## ğŸ› ï¸ ç®¡ç†å‘½ä»¤

```bash
# å•Ÿå‹•æœå‹™
docker-compose up -d

# åœæ­¢æœå‹™
docker-compose down

# æŸ¥çœ‹æ—¥èªŒ
docker-compose logs -f app

# é‡å•Ÿæœå‹™
docker-compose restart

# é‡å»ºä¸¦å•Ÿå‹•ï¼ˆç•¶æ›´æ–°ä»£ç¢¼å¾Œï¼‰
docker-compose up -d --build

# æŸ¥çœ‹æœå‹™ç‹€æ…‹
docker-compose ps

# é€²å…¥å®¹å™¨ï¼ˆdebug ç”¨ï¼‰
docker-compose exec app bash
```

## ğŸ’» æœ¬åœ°é–‹ç™¼ï¼ˆä¸ä½¿ç”¨ Dockerï¼‰

å¦‚æœä½ æƒ³åœ¨æœ¬åœ°ç›´æ¥é‹è¡Œï¼ˆä¸æ¨è–¦æ–°æ‰‹ï¼‰ï¼š

```bash
# 1. å‰µå»ºè™›æ“¬ç’°å¢ƒ
python -m venv venv
source venv/bin/activate  # Mac/Linux
# æˆ– venv\Scripts\activate  # Windows

# 2. å®‰è£ä¾è³´
pip install -r requirements.txt

# 3. å•Ÿå‹•æœå‹™
python scripts/start.py
```

## ğŸ”Œ API ç¯„ä¾‹

### å®Œæ•´å·¥ä½œæµç¨‹ API

```python
import requests

BASE_URL = "http://localhost:8000/api/v1/testset"

# éšæ®µ 1: ç”Ÿæˆè§’è‰²
personas_response = requests.post(f"{BASE_URL}/workflow/generate-personas", json={
    "scenario_description": "ä¸€å€‹æˆ¿åœ°ç”¢åª’åˆå¹³å°...",
    "num_personas": 10,
    "output_folder": "/app/outputs/real_estate",
    "model_provider": "openai",
    "model_name": "gpt-4"
})
personas_result = personas_response.json()

# éšæ®µ 2: ç”Ÿæˆæ–‡ä»¶
documents_response = requests.post(f"{BASE_URL}/workflow/generate-documents", json={
    "scenario_description": "ä¸€å€‹æˆ¿åœ°ç”¢åª’åˆå¹³å°...",
    "num_documents": 10,
    "output_folder": "/app/outputs/real_estate",
    "scenario_name": personas_result["scenario_name"],
    "model_provider": "openai",
    "model_name": "gpt-4"
})
documents_result = documents_response.json()

# éšæ®µ 3 & 4: ç”Ÿæˆ RAG æ¸¬è©¦é›†
rag_response = requests.post(f"{BASE_URL}/workflow/generate-rag-testset", json={
    "documents_folder": documents_result["documents_folder"],
    "personas_json_path": personas_result["json_file"],
    "output_folder": "/app/outputs/real_estate",
    "scenario_name": personas_result["scenario_name"],
    "model_provider": "openai",
    "model_name": "gpt-3.5-turbo",
    "chunk_size": 5000,
    "chunk_overlap": 200,
    "qa_per_chunk": 3
})
rag_result = rag_response.json()
print(f"ç”Ÿæˆäº† {rag_result['total_qa_pairs']} å€‹ QA pairs")
```

### æŸ¥çœ‹ API æ–‡æª”

è¨ªå• http://localhost:8000/docs æŸ¥çœ‹å®Œæ•´çš„ API æ–‡æª”ï¼ˆSwagger UIï¼‰

## ğŸ¤ è²¢ç»

æ­¡è¿è²¢ç»ï¼è«‹æŸ¥çœ‹ [è²¢ç»æŒ‡å—](CONTRIBUTING.md) æˆ–ç›´æ¥æäº¤ Pull Requestã€‚

## ğŸ“„ License

MIT License - è©³è¦‹ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ™ è‡´è¬

- [RAGAS](https://github.com/explodinggradients/ragas) - RAG è©•ä¼°æ¡†æ¶
- [LangChain](https://github.com/langchain-ai/langchain) - LLM æ‡‰ç”¨æ¡†æ¶
- [LangSmith](https://smith.langchain.com/) - LLM è¿½è¹¤å’Œç›£æ§

---

**å¦‚æœ‰å•é¡Œæˆ–å»ºè­°ï¼Œæ­¡è¿é–‹ Issue æˆ–è¯ç¹«æˆ‘å€‘ï¼**

### èˆŠç‰ˆ API ä½¿ç”¨ç¯„ä¾‹ï¼ˆåƒ…ä¾›åƒè€ƒï¼‰

<details>
<summary>é»æ“Šå±•é–‹æŸ¥çœ‹èˆŠç‰ˆ API ç¯„ä¾‹</summary>

### LLM Judge Evaluation

```bash
# Evaluate with OpenAI judge
curl -X POST "http://localhost:8000/api/v1/evaluation/judge" \
  -H "Content-Type: application/json" \
  -d '{
    "testset_data": [
      {
        "question": "What is machine learning?",
        "ground_truth": "Machine learning is a subset of AI...",
        "llm_response": "Machine learning is a method of data analysis..."
      }
    ],
    "llm_endpoint": "https://api.openai.com/v1/chat/completions",
    "judge_model_type": "openai",
    "judge_model": "gpt-4-turbo-preview",
    "evaluation_criteria": ["accuracy", "factual_correctness"]
  }'

# Evaluate with Gemini judge
curl -X POST "http://localhost:8000/api/v1/evaluation/judge" \
  -H "Content-Type: application/json" \
  -d '{
    "testset_data": [
      {
        "question": "What is machine learning?",
        "ground_truth": "Machine learning is a subset of AI...",
        "llm_response": "Machine learning is a method of data analysis..."
      }
    ],
    "llm_endpoint": "https://api.openai.com/v1/chat/completions",
    "judge_model_type": "gemini",
    "judge_model": "gemini-pro",
    "evaluation_criteria": ["accuracy", "factual_correctness"]
  }'

# Evaluate with Ollama judge
curl -X POST "http://localhost:8000/api/v1/evaluation/judge" \
  -H "Content-Type: application/json" \
  -d '{
    "testset_data": [
      {
        "question": "What is machine learning?",
        "ground_truth": "Machine learning is a subset of AI...",
        "llm_response": "Machine learning is a method of data analysis..."
      }
    ],
    "llm_endpoint": "https://api.openai.com/v1/chat/completions",
    "judge_model_type": "ollama",
    "judge_model": "llama2",
    "evaluation_criteria": ["accuracy", "factual_correctness"]
  }'
```

## Multi-Model Judge Support

The platform supports multiple LLM models as judges for evaluation:

### Supported Judge Models

1. **OpenAI Models**
   - `gpt-4-turbo-preview`
   - `gpt-4`
   - `gpt-3.5-turbo`

2. **Google Gemini Models**
   - `gemini-pro`
   - `gemini-pro-vision`

3. **Ollama Models** (Local)
   - `llama2`
   - `codellama`
   - `mistral`
   - `neural-chat`

4. **Hugging Face Models**
   - `microsoft/DialoGPT-medium`
   - `facebook/blenderbot-400M-distill`

### Judge Model Selection

You can specify the judge model in your evaluation requests:

```json
{
  "judge_model_type": "gemini",
  "judge_model": "gemini-pro",
  "evaluation_criteria": ["accuracy", "factual_correctness"]
}
```

## Available Metrics

### RAG Metrics
- `accuracy`: Overall accuracy compared to ground truth
- `factual_correctness`: Factual accuracy of responses
- `precision`: Precision of information
- `recall`: Recall of relevant information
- `f1`: Harmonic mean of precision and recall
- `response_relevancy`: Relevance of responses to questions
- `faithfulness`: Faithfulness to provided context
- `context_precision`: Precision of retrieved context
- `context_recall`: Recall of relevant context
- `answer_relevancy`: Relevance of answers
- `answer_correctness`: Correctness of answers
- `answer_similarity`: Similarity to ground truth
- `semantic_similarity`: Semantic similarity
- `bleu_score`: BLEU score for text similarity
- `rouge_score`: ROUGE score for text similarity
- `exact_match`: Exact match with ground truth

### Agent Metrics
- `average_turn`: Average number of turns per conversation
- `success_rate`: Success rate of agent tasks
- `tool_call_accuracy`: Accuracy of tool calls
- `agent_goal_accuracy`: Accuracy in achieving goals
- `topic_adherence`: Adherence to conversation topic

## Project Structure

```
llm-eval-hub/
â”œâ”€â”€ ai/                     # AI evaluation modules
â”‚   â”œâ”€â”€ core/              # Core evaluation logic
â”‚   â”‚   â”œâ”€â”€ evaluator.py   # Main evaluator using RAGAS
â”‚   â”‚   â””â”€â”€ llm_judge.py   # LLM judge system
â”‚   â””â”€â”€ testset/           # Testset generation
â”‚       â””â”€â”€ generator.py   # Testset generator service
â”œâ”€â”€ api/                   # FastAPI application
â”‚   â”œâ”€â”€ routes/           # API routes
â”‚   â”‚   â”œâ”€â”€ evaluation.py # Evaluation endpoints
â”‚   â”‚   â”œâ”€â”€ testset.py    # Testset endpoints
â”‚   â”‚   â””â”€â”€ health.py     # Health check endpoints
â”‚   â”œâ”€â”€ middleware.py     # Custom middleware
â”‚   â””â”€â”€ main.py          # FastAPI app
â”œâ”€â”€ configs/             # Configuration
â”‚   â””â”€â”€ settings.py     # Application settings
â”œâ”€â”€ data/               # Data storage
â”‚   â”œâ”€â”€ raw/           # Raw data
â”‚   â””â”€â”€ processed/     # Processed data
â”œâ”€â”€ docs/              # Documentation
â”œâ”€â”€ tutorials/         # Tutorial examples
â”œâ”€â”€ examples/          # Usage examples
â”œâ”€â”€ outputs/          # Inference results
â”œâ”€â”€ artifacts/        # Model artifacts
â”œâ”€â”€ results/          # Evaluation results
â”œâ”€â”€ tests/            # Test files
â”œâ”€â”€ docker-compose.yml # Docker configuration
â”œâ”€â”€ Dockerfile        # Docker image
â”œâ”€â”€ requirements.txt  # Python dependencies
â”œâ”€â”€ pyproject.toml   # Project configuration
â””â”€â”€ Makefile         # Build commands
```

## Development

### Running Tests

```bash
make test
```

### Code Formatting

```bash
make format
```

### Linting

```bash
make lint
```

### Building Docker Image

```bash
make build
```

## Configuration

The application can be configured through environment variables. See `env.example` for all available options.

### Key Configuration Options

- `LANGSMITH_API_KEY`: LangSmith API key for evaluation tracking
- `OPENAI_API_KEY`: OpenAI API key for LLM judge
- `GEMINI_API_KEY`: Google Gemini API key for judge evaluation
- `OLLAMA_BASE_URL`: Ollama server URL (default: http://localhost:11434)
- `HUGGINGFACE_API_KEY`: Hugging Face API key for judge evaluation
- `SECRET_KEY`: Secret key for JWT token generation and session management
- `DATABASE_URL`: Database connection string
- `DEFAULT_EVALUATION_TIMEOUT`: Timeout for evaluations (seconds)
- `MAX_CONCURRENT_EVALUATIONS`: Maximum concurrent evaluations

## Monitoring

The application includes comprehensive monitoring:

- **Health Checks**: `/health`, `/health/detailed`, `/health/ready`, `/health/live`
- **Metrics**: Prometheus-compatible metrics endpoint
- **Structured Logging**: JSON-formatted logs with request tracing
- **Error Tracking**: Comprehensive error handling and logging

</details>